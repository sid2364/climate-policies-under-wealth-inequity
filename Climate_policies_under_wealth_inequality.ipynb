{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef94ee07-f43f-4b27-bf45-253a069cb66f",
   "metadata": {},
   "source": [
    "## Climate Policies Under Wealth Inequality\n",
    "This notebook is meant to reproduce the results shown in the paper \"Climate policies under wealth inequality\", linked here: https://www.pnas.org/doi/10.1073/pnas.1323479111. \n",
    "\n",
    "#### Abstract of the paper:\n",
    "(verbatim) Taming the planet’s climate requires cooperation. Previous failures to reach consensus in climate summits have been attributed, among other factors, to conflicting policies between rich and poor countries, which disagree on the implementation of mitigation measures. Here we implement wealth inequality in a threshold public goods dilemma of cooperation in which players also face the risk of potential future losses. We consider a population exhibiting an asymmetric distribution of rich and poor players that reflects the present-day status of nations and study the behavioral interplay between rich and poor in time, regarding their willingness to cooperate. Individuals are also allowed to exhibit a variable degree of homophily, which acts to limit those that constitute one’s sphere of influence. Under the premises of our model, and in the absence of homophily, comparison between scenarios with wealth inequality and without wealth inequality shows that the former leads to more global cooperation than the latter. Furthermore, we find that the rich generally contribute more than the poor and will often compensate for the lower contribution of the latter. Contributions from the poor, which are crucial to overcome the climate change dilemma, are shown to be very sensitive to homophily, which, if prevalent, can lead to a collapse of their overall contribution. In such cases, however, we also find that obstinate cooperative behavior by a few poor may largely compensate for homophilic behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07484f9b-3591-4de2-92bd-33557c70b8a9",
   "metadata": {},
   "source": [
    "To achieve the results of the paper, we have created a Public Goods Game (PGG) to model the climate policies game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeeb0348-a43b-4268-bd08-ce348f8ad089",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install egttools numpy matplotlib > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a9d65e-caf4-4918-a4d1-d5faa64059f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union, List\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from egttools.games import AbstractNPlayerGame\n",
    "from egttools.analytical import PairwiseComparison\n",
    "from egttools import sample_simplex, calculate_nb_states\n",
    "from egttools.utils import calculate_stationary_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ce764f-ff4f-4ea3-9ea0-02acf82a60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ClimateChangeThresholdPGG import ClimateChangeThresholdPGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad79494-9d64-4e74-abff-3ed2cc510712",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_size = 6\n",
    "population_size = 200\n",
    "b_R = 2.5\n",
    "b_P = 0.625\n",
    "c = 0.1\n",
    "r = 0.5\n",
    "h = 0.1\n",
    "M = 3\n",
    "beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ad6e7-de62-4b18-8395-e8659b89e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClimateChangeThresholdPGG(AbstractNPlayerGame):\n",
    "    def __init__(self,\n",
    "                 group_size: int,  # N - Number of individuals that be sampled to play\n",
    "                 population_size_Z: int,  # Total population - Will be divided 80-20% (Poor-Rich)\n",
    "                 b_R: float,  # Endowment of the rich\n",
    "                 b_P: float,  # Endowment of the poor\n",
    "                 c: float,  # Fraction of endowment Cs give\n",
    "                 r: float,  # Risk factor [0, 1]\n",
    "                 h: float,  # Homophily [0, 1] - \"Like imitates like\" factor\n",
    "                 M: float,  # Factor which will calculate success (M x c x avg(b) should be met)\n",
    "                 beta: float  # Intensity of selection for imitating strategies\n",
    "                 ):\n",
    "\n",
    "        AbstractNPlayerGame.__init__(self, 4, group_size)  # (self, nb_strategies, group_size)\n",
    "\n",
    "        self.nb_strategies_ = 4\n",
    "        self.strategies = [\"C_R\", \"D_R\", \"C_P\", \"D_P\"]  # Cooperate/Defect_Rich/Poor\n",
    "\n",
    "        self.group_size_ = group_size\n",
    "        self.N = group_size  # These two^ are the same, just being more consistent with the paper\n",
    "\n",
    "        assert b_R > b_P, \"The Marxists are back! Rise of the proletariat!\"\n",
    "        self.b_R = b_R\n",
    "        self.b_P = b_P\n",
    "\n",
    "        self.c = c\n",
    "        self.r = r\n",
    "        self.h = h\n",
    "        self.M = M\n",
    "        self.beta = beta\n",
    "\n",
    "        # Proportions of rich and poor\n",
    "        self.rich_ratio = 0.2  # 20% rich\n",
    "        self.poor_ratio = 0.8  # 80% poor\n",
    "\n",
    "        self.nb_group_configurations_ = self.nb_group_configurations()\n",
    "        self.calculate_payoffs()\n",
    "\n",
    "    def play(self,\n",
    "             group_composition: Union[List[int], np.ndarray],\n",
    "             game_payoffs: np.ndarray):\n",
    "        \"\"\"\n",
    "        Simulates one round of the game\n",
    "        \"\"\"\n",
    "        game_payoffs[:] = 0.0\n",
    "\n",
    "        # Calculate total contributions\n",
    "        rich_cooperators = group_composition[0]\n",
    "        poor_cooperators = group_composition[1]\n",
    "        rich_defectors = group_composition[2]\n",
    "        poor_defectors = group_composition[3]\n",
    "\n",
    "        total_contributions = (rich_cooperators * self.b_R * self.c +\n",
    "                               poor_cooperators * self.b_P * self.c)\n",
    "        group_size = group_composition.sum()\n",
    "\n",
    "        # Calculate average endowment (b) for the group\n",
    "        total_endowment = (rich_cooperators + rich_defectors) * self.b_R + \\\n",
    "                          (poor_cooperators + poor_defectors) * self.b_P\n",
    "        average_endowment = total_endowment / group_size\n",
    "\n",
    "        # Check if the threshold is met\n",
    "        threshold = self.M * self.c * average_endowment\n",
    "        success = total_contributions >= threshold\n",
    "        disaster = np.random.rand() < self.r if not success else False\n",
    "\n",
    "        # Assign payoffs based on contributions and disaster\n",
    "        for idx, count in enumerate(group_composition):\n",
    "            if count > 0:\n",
    "                if idx == 0:  # Rich cooperators\n",
    "                    loss = self.b_R * (1 - self.c) if disaster else 0\n",
    "                    payoff = self.b_R * self.c * success - loss\n",
    "                elif idx == 1:  # Poor cooperators\n",
    "                    loss = self.b_P * (1 - self.c) if disaster else 0\n",
    "                    payoff = self.b_P * self.c * success - loss\n",
    "                elif idx == 2:  # Rich defectors\n",
    "                    loss = self.b_R if disaster else 0\n",
    "                    payoff = self.b_R - loss\n",
    "                elif idx == 3:  # Poor defectors\n",
    "                    loss = self.b_P if disaster else 0\n",
    "                    payoff = self.b_P - loss\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown strategy index: {idx}\")\n",
    "\n",
    "                # Update payoffs for this strategy\n",
    "                game_payoffs[idx] += payoff\n",
    "\n",
    "    def calculate_payoffs(self):\n",
    "        payoffs_container = np.zeros(self.nb_strategies_)\n",
    "        for i in range(self.nb_group_configurations_):\n",
    "            group_composition = sample_simplex(i, self.group_size_, self.nb_strategies_)\n",
    "            print(group_composition)\n",
    "            self.play(group_composition, payoffs_container)\n",
    "            for strategy_idx, payoff in enumerate(payoffs_container):\n",
    "                self.update_payoff(strategy_idx, i, payoff)\n",
    "            print(payoffs_container)\n",
    "\n",
    "            payoffs_container[:] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b95e3585-50d2-4a1f-ae43-6af5affa534f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 0 0 0]\n",
      "[0.25 0.   0.   0.  ]\n",
      "[5 1 0 0]\n",
      "[0.25   0.0625 0.     0.    ]\n",
      "[5 0 1 0]\n",
      "[0.25 0.   2.5  0.  ]\n",
      "[5 0 0 1]\n",
      "[0.25  0.    0.    0.625]\n",
      "[4 2 0 0]\n",
      "[0.25   0.0625 0.     0.    ]\n",
      "[4 1 1 0]\n",
      "[0.25   0.0625 2.5    0.    ]\n",
      "[4 1 0 1]\n",
      "[0.25   0.0625 0.     0.625 ]\n",
      "[4 0 2 0]\n",
      "[0.25 0.   2.5  0.  ]\n",
      "[4 0 1 1]\n",
      "[0.25  0.    2.5   0.625]\n",
      "[4 0 0 2]\n",
      "[0.25  0.    0.    0.625]\n",
      "[3 3 0 0]\n",
      "[0.25   0.0625 0.     0.    ]\n",
      "[3 2 1 0]\n",
      "[0.25   0.0625 2.5    0.    ]\n",
      "[3 2 0 1]\n",
      "[0.25   0.0625 0.     0.625 ]\n",
      "[3 1 2 0]\n",
      "[0.25   0.0625 2.5    0.    ]\n",
      "[3 1 1 1]\n",
      "[0.25   0.0625 2.5    0.625 ]\n",
      "[3 1 0 2]\n",
      "[0.25   0.0625 0.     0.625 ]\n",
      "[3 0 3 0]\n",
      "[0.  0.  2.5 0. ]\n",
      "[3 0 2 1]\n",
      "[0.25  0.    2.5   0.625]\n",
      "[3 0 1 2]\n",
      "[0.25  0.    2.5   0.625]\n",
      "[3 0 0 3]\n",
      "[0.25  0.    0.    0.625]\n",
      "[2 4 0 0]\n",
      "[0.25   0.0625 0.     0.    ]\n",
      "[2 3 1 0]\n",
      "[0.25   0.0625 2.5    0.    ]\n",
      "[2 3 0 1]\n",
      "[0.25   0.0625 0.     0.625 ]\n",
      "[2 2 2 0]\n",
      "[0.25   0.0625 2.5    0.    ]\n",
      "[2 2 1 1]\n",
      "[0.25   0.0625 2.5    0.625 ]\n",
      "[2 2 0 2]\n",
      "[0.25   0.0625 0.     0.625 ]\n",
      "[2 1 3 0]\n",
      "[-2.25   -0.5625  0.      0.    ]\n",
      "[2 1 2 1]\n",
      "[-2.25   -0.5625  0.      0.    ]\n",
      "[2 1 1 2]\n",
      "[0.25   0.0625 2.5    0.625 ]\n",
      "[2 1 0 3]\n",
      "[0.25   0.0625 0.     0.625 ]\n",
      "[2 0 4 0]\n",
      "[0.  0.  2.5 0. ]\n",
      "[2 0 3 1]\n",
      "[0.    0.    2.5   0.625]\n",
      "[2 0 2 2]\n",
      "[-2.25  0.    0.    0.  ]\n",
      "[2 0 1 3]\n",
      "[0.25  0.    2.5   0.625]\n",
      "[2 0 0 4]\n",
      "[0.25  0.    0.    0.625]\n",
      "[1 5 0 0]\n",
      "[0.25   0.0625 0.     0.    ]\n",
      "[1 4 1 0]\n",
      "[0.25   0.0625 2.5    0.    ]\n",
      "[1 4 0 1]\n",
      "[0.25   0.0625 0.     0.625 ]\n",
      "[1 3 2 0]\n",
      "[-2.25   -0.5625  0.      0.    ]\n",
      "[1 3 1 1]\n",
      "[0.25   0.0625 2.5    0.625 ]\n",
      "[1 3 0 2]\n",
      "[0.25   0.0625 0.     0.625 ]\n",
      "[1 2 3 0]\n",
      "[0.  0.  2.5 0. ]\n",
      "[1 2 2 1]\n",
      "[-2.25   -0.5625  0.      0.    ]\n",
      "[1 2 1 2]\n",
      "[-2.25   -0.5625  0.      0.    ]\n",
      "[1 2 0 3]\n",
      "[0.25   0.0625 0.     0.625 ]\n",
      "[1 1 4 0]\n",
      "[-2.25   -0.5625  0.      0.    ]\n",
      "[1 1 3 1]\n",
      "[0.    0.    2.5   0.625]\n",
      "[1 1 2 2]\n",
      "[-2.25   -0.5625  0.      0.    ]\n",
      "[1 1 1 3]\n",
      "[-2.25   -0.5625  0.      0.    ]\n",
      "[1 1 0 4]\n",
      "[0.25   0.0625 0.     0.625 ]\n",
      "[1 0 5 0]\n",
      "[0.  0.  2.5 0. ]\n",
      "[1 0 4 1]\n",
      "[0.    0.    2.5   0.625]\n",
      "[1 0 3 2]\n",
      "[0.    0.    2.5   0.625]\n",
      "[1 0 2 3]\n",
      "[-2.25  0.    0.    0.  ]\n",
      "[1 0 1 4]\n",
      "[0.    0.    2.5   0.625]\n",
      "[1 0 0 5]\n",
      "[0.    0.    0.    0.625]\n",
      "[0 6 0 0]\n",
      "[0.     0.0625 0.     0.    ]\n",
      "[0 5 1 0]\n",
      "[0.     0.0625 2.5    0.    ]\n",
      "[0 5 0 1]\n",
      "[0.     0.0625 0.     0.625 ]\n",
      "[0 4 2 0]\n",
      "[0.  0.  2.5 0. ]\n",
      "[0 4 1 1]\n",
      "[0.    0.    2.5   0.625]\n",
      "[0 4 0 2]\n",
      "[0.     0.0625 0.     0.625 ]\n",
      "[0 3 3 0]\n",
      "[ 0.     -0.5625  0.      0.    ]\n",
      "[0 3 2 1]\n",
      "[0.    0.    2.5   0.625]\n",
      "[0 3 1 2]\n",
      "[0.    0.    2.5   0.625]\n",
      "[0 3 0 3]\n",
      "[ 0.     -0.5625  0.      0.    ]\n",
      "[0 2 4 0]\n",
      "[ 0.     -0.5625  0.      0.    ]\n",
      "[0 2 3 1]\n",
      "[ 0.     -0.5625  0.      0.    ]\n",
      "[0 2 2 2]\n",
      "[ 0.     -0.5625  0.      0.    ]\n",
      "[0 2 1 3]\n",
      "[0.    0.    2.5   0.625]\n",
      "[0 2 0 4]\n",
      "[ 0.     -0.5625  0.      0.    ]\n",
      "[0 1 5 0]\n",
      "[ 0.     -0.5625  0.      0.    ]\n",
      "[0 1 4 1]\n",
      "[ 0.     -0.5625  0.      0.    ]\n",
      "[0 1 3 2]\n",
      "[0.    0.    2.5   0.625]\n",
      "[0 1 2 3]\n",
      "[ 0.     -0.5625  0.      0.    ]\n",
      "[0 1 1 4]\n",
      "[0.    0.    2.5   0.625]\n",
      "[0 1 0 5]\n",
      "[ 0.     -0.5625  0.      0.    ]\n",
      "[0 0 6 0]\n",
      "[0.  0.  2.5 0. ]\n",
      "[0 0 5 1]\n",
      "[0.    0.    2.5   0.625]\n",
      "[0 0 4 2]\n",
      "[0.    0.    2.5   0.625]\n",
      "[0 0 3 3]\n",
      "[0. 0. 0. 0.]\n",
      "[0 0 2 4]\n",
      "[0. 0. 0. 0.]\n",
      "[0 0 1 5]\n",
      "[0. 0. 0. 0.]\n",
      "[0 0 0 6]\n",
      "[0.    0.    0.    0.625]\n"
     ]
    }
   ],
   "source": [
    "climate_pgg = ClimateChangeThresholdPGG(group_size, population_size, b_R, b_P, c, r, h, M, beta)\n",
    "# evolver = PairwiseComparison(M, climate_pgg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Learning Dynamics Env)",
   "language": "python",
   "name": "learning_dynamics_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
